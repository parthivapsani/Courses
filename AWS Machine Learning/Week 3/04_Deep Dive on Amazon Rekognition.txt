Hi. My name is
[inaudible] and I'm a senior solutions
architect at AWS. Welcome to the course on
Amazon Rekognition Deep Dive. So in this course,
we will talk about some of the core features
of Amazon Rekognition, then we'll cover the API and how you can use those to
build applications, we will then cover some
of the use cases along with reference architectures on how to build those use cases, and we will then wrap up the course of its on
the best practices. Before we talk about
Amazon Rekognition, let's quickly talk about what is computer vision and
why is it important. So here's an example
image but as a human, we can easily see that
there is a person, the person is on a mountain bike, and we see there are
rocks there and so on. But if you think about
it behind the scene, only what computer see
is a bunch of numbers. So if you take any
pixel of this image, all we have is behind
the seeing a bunch of numbers which computers have no idea what is in that image. Whereas as human, it is very easy for us to see
different objects, what is in the scene, and
all activities and so on. So the goal here is using a machine learning models to be able to detect what
are the objects, what are the scene,
what are some of the activities in these images, and be able to make
sense of this content. To detect different
objects, different scene, and different activities, generally you have to build
a machine learning models. To do that, you need
lots and lots of data. So for example, if you want to detect cats in an image or if
there are dogs in an image, you need lots and
lots of pictures of cats and dogs and that's where application services more specifically
recognition comes in. So before we talk
about Rekognition, let's quickly talk about the Amazons and Machine
Learning stack. These are the
services that Amazon has to make Machine Learning and AI really easy and basically democratize AI and ML
for organizations. At the bottom layer after stack, we have frameworks
and interfaces. So this is where you can use things like AWS deep learning AMI and run any of the
frameworks that you want, things like whether it's TensorFlow and whether
it's MXNet, Caffe2, you name it, you can run any of those frameworks and build
your machine learning models. At the mid-level of the stack, is platform services and that's where we have
services like SageMaker. SageMaker makes it very easy
for developers and data scientists to build train and then host their
machine learning models. At the top layer of the stack, we have application services. So this is where we at
Amazon built and train machine-learning models
whether this is for computer vision or language
services and so on, things like transcribe and
translate and we deploy those machine learning models in highly available and highly
scalable environment, and wrap those models with APIs and that's how we
expose those models to you. So as a developer, without knowing anything about
AI and machine learning, you can take advantage
of these services and build pretty powerful
smart applications. Before we look at some
of the core features, here are some of the customers of Amazon recognition who are leveraging the service today for a variety of use cases and, we'll talk a little
bit more about some of these use cases farther
down in the course. Let's now dig into Rekognition as this
course is focused on computer vision use cases
and see what are some of the core features that
Rekognition provide. So Amazon Rekognition is
a deep learning base, image, and video
analysis service. When we talk about
image analysis, it can do things like
object and scene detection, it can do facial analysis,
facial recognition, it can do things like
celebrity Rekognition as well as detect unsafe
content and images, it also has the ability
to detect text in images. When we talk about video, again Amazon Rekognition can not only detect objects and seen like in images but it can also have
the time axis information. So it can detect
activity for which usually you need more
than just one frame and usually you have to
look at a couple of frames to see what is going on at that time in that
video for example. Along the same lines as image, a Rekognition video can do things like celebrity
Rekognition, it can do content
moderation along with face detection as
well as pathing, where it can track the
path that different people took in that video and there's very interesting use
cases that you can build. For example, looking at
sounder sports videos and we'll talk about some of
those down in the use cases. Let's now dig through each of these features and talk a little bit more about
how these work. So the first one is object
and scene detection. In this case, we take an image and we send it
to Amazon Rekognition. Rekognition then
look at the image, detect different
objects, what is in the scene and return
us a list of labels. So for example in this case, you see image on the left, we get different labels like
chair or a living room, coffee table, and so on. Whereas on the image
on the right it's able to detect that there
is a swimming pool, there is water, and so on. Generally, when you're building media library which has a lot of unstructured content thing like images and videos in the past, usually you had to tag every single image
or video that gets into the media library to
make library more searchable and also to be able to easily
find the content. By using Amazon
Rekognition scenes, now Rekognition returns us all the information
about that scene, it's very easy to then automatically tag those images
and video and make your media library easily
accessible instead of manually tagging
every single object that comes into
the Media Library. Rekognition provides
facial analysis and so that's very you can
send for example an image. It can analyze the different
facial characteristics along different dimensions. So things like demographic data, where it can tell you the
age range of the person, it can detect gender
of the person, it can provide information
about facial landmarks. So for example, where
the person's left eye, right eye is information
about nose and so on. Rekognition return
you information about the brightness
or the sharpness of the image as well as other attributes including
how the emotions are, whether the person is
happy, they're smiling, they're surprised, whether
their eyes are open, whether they have beard
or not, and so on. Other information that
Rekognition provides you as part of the facial analysis
is the official pores which can help you understand and see which direction
the person is looking at and so on by taking advantage of the
attributes like pitch, roll, and yaw for example. Facial detection and analysis is able to detect up to a 100 faces. So for example in this case, you see that is an
image of a crowd and Rekognition can easily detect up to a 100 faces in that crowd. Another feature that
Amazon Rekognition support is called face comparison. So in this case, you can
take two images and ask Rekognition to compare if those are images of the
same person or not. So for example, you can
see image on the left, the individual is with some
other people and the image on the right is another
picture of the same person. But if you see the
image on the right, the person doesn't have
beard for example. Where he has more facial hair
on the image on the left, my Rekognition is able to compare those and tell you a
confidence level saying, it looks like there's a
93 percent similarity in these two images
for these two faces. Whereas the other two
faces in the images don't necessarily have any similarity and the score is zero percent. Another feature that Rekognition provides is called
facial Rekognition. So in this case you're not
just comparing two faces. In this case what
we're saying is, we want to build a
collection of faces. So there could be millions
of faces that you can index in that collection and
then for any incoming face, if you want to
compare it and see if there is a match in
that collection. That's something Rekognition
can easily do even when you have collection
of millions of faces in under a second. Another core feature that I condition provide is
content moderation. So in this case, it can
easily detect if there is any explicit or
suggestive content for example in the
image or video. We don't tell you whether
it's explicit or suggestive, data are basically two
levels of hierarchy. So you got a top level category, where you get to know
whether the content it has explicit or
suggestive content. Then it can further give you more details at the second
level whether for example, there is nudity and so on. The reason we give you
these two categories is because depending on the countries or laws of
our country or region, one type of content
might be okay, if we added in another region the same content may not be okay. So for example, by detecting that there
is revealing clothes, maybe that content is okay
for certain type of website and certain type
of region and not maybe in another type
of region and so on. So you get to control based
on what is in that content. Another core feature that
Rekognition provides, is celebrity air recognition. So in this case again, you can send an image or you
can send a video and we have hundreds and thousands
of famous individuals whether they are politicians, sportsman, actors,
actresses, and so on, and you can easily get those
recognize the inner content. Another core feature of
Rekognition is detect text. So in this case for example, you can see an image with the piece of paper
but some texts on our table and Rekognition is able to easily then
detect that text. In fact, it returned to all those different
lines then you can build in a very interesting
applications using that. Another feature is the pathing. This is where you can detect different path that people
took in a video for example. So in this screen, you
can see that it is soccer players and so by
looking at that video, you can detect there are
different players in the game and these are the different
paths that people took. Some of that information
can easily be used then for interesting use
cases like building, highlighting, real or many of the other similar use cases. These are some of the core features that
Rekognition provide. Now, let's quickly look at the
Rekognition API and see as a developer what does it take to take advantage
of these features. So we'll look at the first API which is called the tech labels. This is to get different
labels for an image. So you can see it's
pretty straight forward. The way it works is you call DetectLabels and you pass it's request which has information about the image that you
want to do the analysis for. You can either send that
raw bytes or you can send the information
about the S3 bucket where that image resides. In addition, you
can ask how many of the labels that you want to
get returned in that result, as well as what is the
minimum confidence level. So if you set the
confidence level for example to 80 percent, then we will only return you the labels which we are certain that that level has a confidence level of
80 percent or above. Once you call the tech labels, you get the response
back where we return you list of labels along with the confidence level for
each of those labels. Now let's see this was
the request for image, how would you do the same
thing for the video API? So if you're sending the video, in this case again the
request is quite similar, they use sender information about the S3 bucket where the video is. One thing that additional
that I want to highlight here is that we would
call SNSTopicArn. The reason we have
that is because the Video API is asynchronous. So in this case, instead of
calling DetectLabel you call StartLabelDetection
which basically starts a job or to analyze the video. As part of that
StartLabelDetection, you can pass us information
about an SNS topic. So as the job completes, we can then call
that SNS topic to know let you know that
the job is complete. So instead of manually pooling to know whether the job
is complete or not, you can just take advantage
of the SNSTopicArn feature. Menu called StartLabelDetection
the response returns you a job
ID and then based on that you can call
GetLabelDetection which returns you obviously
a couple of attributes. One of those is the
status message that can help you see whether
the job is complete or not. In addition, the core
information that is labels. For the image API, you saw we pretty much got
the list of labels back. In case of video, there is an additional
attribute called Timestamp, so it can tell you time
one millisecond or five millisecond in
the frame these are the objects and or activities and things that
recognition found. Most of the other APIs they
all follow the same pattern. So pretty much for image, you will pass the
image information and get the results
back, for the video, you will start a job, get a job ID back, and then based on that you can get the rest of the information. So the API calls usually we categorize them into
different buckets: one is the non-storage API
operations and the other are storage-based
API operations. So all the things like
DetectLabel, compare faces, and so on those are all done using non-storage API operations. Whereas when you
build a collection and you do things
like index faces, you want to list all the faces. Those are the ones that you
are doing on that storage which means the
collection that you created for all the
faces you have. Now with the understanding
of core features as well as how does the API work, let's look at some of the
use cases that we have seen customers are using
Amazon Rekognition. So the first one I'll talk about the royal wedding
coverage of Sky News. So in this case, when the
royal wedding happen, the Sky News built a feature
called Who is Who Live. They identify the guests as they were arriving
at the event. Then you can see the UI in the screen where it was showing the screen captions and basically showed the relation of the
guests to the royal couple. Here are a few other
screenshots of the UI how it looked like
as the person is arriving, so they were able to take
advantage of the things we mentioned like
Rekognition collection, indexing faces, and then as
the guest arrives comparing that face against
their collection and find out who that person is. So you can see it's pretty easy to build a use
cases like these. Another use case we have seen is for example sports
and media tagging. So instead of manually
going through a game, a recording of a
game and manually tagging a bunch of
frames across the game, you can pretty much take a video, use different
Rekognition features for example use pathing to detect different players and the path that it took
during the game, and maybe use detect
text for example to look at the scoreboard and find out when
the score change. That way by detecting different objects,
different activities, you can build for example
reel of highlights and so on. Instead of doing it manually, just easier to do that
by using a Rekognition and detecting all these objects and activities in the game. Another use case we have seen across in public safety
and they're companies like Marinus Analytics have Build Machine Learning
Analytics platform for law enforcement to combat human trafficking
and really reduce the time and effort to
identify and rescue victims. Another common use case is across many websites
or social networks, where they want to to analyze
the user-generated content. So whether it's an e-commerce
website where people are uploading product
reviews or this is a social networking site where people are uploading for example their pictures as
their profile picture, and you want to make sure
that the picture is of a person as well as what
is the activity going on, if there is any explicit or suggestive content in that image. So just by taking advantage
of the recognition APIs, you can easily validate that a user-generated content
before it gets published. The really cool thing here
is that you can easily do that by automat the process by using Rekognition
API instead of manually validating all the images or the video that gets uploaded. Let's see how you can build
a solution like this on AWS. So it's pretty straightforward. The image gets uploaded to S3 which can then trigger
a Lambda function. Lambda function can then
call stuff functions which is AWS service to build
really powerful workflows. Stuff function can then execute different Lambda
functions for you. So for example, the first
Lambda function can call detect faces and detect all the
faces that are in that image. That's how you can find out if there are any faces
in the image or if the uploaded image is maybe for dog or another
animal or so on. Another Lambda
function for example can call recognized celebrities. That's how you can identify if the profile picture
that for example is somebody uploading a
celebrities picture or is this some other person. Another Lambda
function can then call moderation API for example,
detect moderation labels. That's how you can
find out if there is any explicit or suggestive
content in there. Then you can take advantage
of the other APIs as well depending on if any of
the images will get uploaded, if any of those images
are duplicates, and so on and then you can
filter those based on that. All of that metadata can
then be stored in DynamoDB as well as something like Elasticsearch to make it
searchable and so on. Another common use case is where you can do chat moderation. So for example, if during the chat any of the
images get uploaded, you can then call API Gateway, pass that image through API get webpage then goes to AWS lambda. Lambda can then call Rekognition, ask for a modulation labels, if it detect based
on your criteria, if it detect any explicit
or suggestive content, it can then get the
response back and then the chat client can
then delete that image, that's part of that chat message. Another use case is we're
maybe a lot of times we want to find out how long people have to wait in a line. That's where again,
you can have a camera pointed at the lane
and using Rekognition, you can easily detect different people who
are in the line. So in this case, you
are not necessarily recognizing like who
these individuals are, all we're seeing
is that there are different people
standing in the line. Then based on their
position in the frame, you can easily identify
how many people are in the line and
then based on that do some analysis to find
out how long does it usually take for people
to be in that line. To build a solution like that, again you have a
camera which is taking the live picture of the people in the line, called API Gateway. API Gateway can then pass
that image to Lambda. Lambda can store that
image in S3 and then call Amazon recognition to
analyze that image to find out how many persons
are there in that image. That information can then
be stored in something like DynamoDB and then you can do analysis
on top of that. Another use case is where you can use Amazon Rekognition
is for example, for live demographic analysis. So in this case, imagine
you have these stores, they're different people walking as they are in different
parts of the store. Again you can take the
image from the store, call Rekognition API to find out the sentiment analysis of the people as well as the
gender of the people, age, range, and so on. So we have seen customers
talking about where maybe they have a menu in the store for
example, at a restaurant. Depending on if there are more kids into store
or if there are more one of the genders in the store and depending on that, they can then customize
their digital menu board and maybe highlight certain
things for kids and so on. The same architecture can be used for real-time store heatmap. So depending on different
cameras in the store, then you can find
out where there are more people or which
section of the store has more people as well as by their gender and their
age, range, and so on. To build a solution like that, again it's pretty
straightforward. So you have those
live images coming from cameras in the store. They get passed on to API Gateway which then
calls Lambda function, and then it gets stored on S3. You can call Rekognition
to detect faces and that's how you find
out sentiment analysis, all that information which you can then store
in something like DynamoDB or store in Amazon Redshift and use quick site to do
analysis and so on. So those were some
of the use cases, there are large
number of use cases. But given the time constraint, I only picked just a few to
talk about in this course. Now, let's talk about some of the best practices
that you should follow as you're building any of those solutions using
Amazon Rekognition. So first of all,
Rekognition support images in both PNG
and JPEG format. The maximum image size, if the image is stored
in S3 then it's 15 Meg. If you're calling the image
directly as you call API, then that is 5 Meg. If you're using video
analysis in that case, the video format is mp4 and MOV and the video codec
is H264 that we support. The maximum video
size should be 8 Gig. If you have video which
is larger than 8 Gig, you can easily use
other AWS services like Elemental for
example and some of the other services to then
break those videos into smaller pieces and then call Rekognition to
analyze those videos. Some of the other
best practices is when you use collections
for example, those collections are for faces. So do not try to index for example pictures of
cats or dogs and so on, that's not going to work. The maximum number of faces in a single collection today
is up to 20 million, that's a number that we
continue to work on. So as you're building
your application, definitely look at
the documentation to see the current number, even when you have 20 million
faces in a collection, the latency that you get
is still under a second, which is pretty awesome. The maximum number of faces
that get returned as a result and the search API
are up to 4,096. Another best practice
is when you index your images in
Rekognition collection, you should keep those images if you ever need to reindex those. Generally, you might need
to reindex those when we launch for example
maybe next major version. In that case, you might have to reindex those images
because we don't keep the images that you call Rekognition API for
index faces for example. We only extract the
facial features and store those vectors
in our system, and we do not keep the
original image that you send as part of
the index face API. So keeping those images
around will come handy when the face models get
updated and you want to take advantage of those
in your next collection, then you can use
those images and just reindex them in your
Rekognition collection. When you use index faces API, so understand that
detects largest a 100 faces and input those images and adds them
to the specific collection. I've seen sometimes customer
who use that index face, they don't realize
that and for example, if they are indexing a face
for a specific person, they might end up seeing
index this person's face and there might be
a few other people, then all of those images or
all of those additional faces will get index by the name of the person
that you are sending. So when you are indexing a face, you should send usually just one image unless
there is a use case where you might want to tag multiple people by the
same ID or something. Similarly, when you use
search face by image, it detects the largest
face in the image and then searches the specify collection for all the matching faces. So if you are looking to
detect a specific face, just make sure that
search face by image is only going to do that
for the largest face. If you need to do it
for all the faces, then you can easily
use detect faces API first which will return you all the
faces in the image, and then for each phase you can call source faces by image. Another important
best practice is to have very high accuracy
and avoid false positives, make sure faces that you are images you're indexing
our high-quality images. If you have blurry
faces or if you have a face that you are indexing doesn't have a lot
of facial features, for example, maybe it's only
a small part of the face. Then that's where you might run into seeing some false positive. So having images
which are not blurry, which are good quality as
well as faces where you have most of the facial
features visible in that image, will increase the likelihood of detecting the correct faces. I wrap up the course
by talking about media analysis solution which we have available on AWS website. There's a link at the bottom of the slide and there's a
CloudFormation template which creates a bunch of
resources including a Lambda function,
from step functions, and S3 buckets and gives you a nice UI where you can
upload some images and videos and quickly get to see all the metadata that different recognition
API has returned. In addition to that, it creates an
Elasticsearch cluster which can then give you
an option to see how you can build a media library
and then quickly be able to search by using the
different metadata that you get from Rekognition. So you'll see instead of
manually tagging each file, you just upload the file and then Rekognition does all the
magic of detecting what is in those images and videos
and then automatically populating you're
Elasticsearch cluster to be able to find those easily. With that, here's
the list of a couple of resources for
you to get started. You can easily use
different Rekognition APIs, either using a variety of
programming languages, whether it's Python,
Java, Node.js, Csharp, Go, and so
on or you can use our mobile SDK for platforms
like Android and iOS, and easily get started by
calling Rekognition and build really powerful smart
computer vision applications. Thank you for watching
the course on Amazon Rekognition Deep Dive, my name is [inaudible] and I hope you check out some
of our other courses.